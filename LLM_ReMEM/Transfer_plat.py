from fastapi import FastAPI, HTTPException, BackgroundTasks
import logging
import time
from datetime import datetime
from pydantic import BaseModel
from typing import List, Optional
import requests
import config
from Memory_Module import Memory_saving, Memory_checking, get_memory_with_context

app = FastAPI()

# LM Studio æœåŠ¡å™¨åœ°å€
LM_STUDIO_API_URL = config.LM_Studio_API

# ä½ çš„ä¸¤ä¸ª LLMï¼ˆä¸€ä¸ªç”¨äºè®°å¿†å¤„ç†ï¼Œä¸€ä¸ªç”¨äºè¯­è¨€è¾“å‡ºï¼‰
MEM_Model = config.MEMMORY_Model  # è®°å¿†å¤„ç†æ¨¡å‹
LLM_Model = config.MAIN_Model     # è¯­è¨€å¤„ç†æ¨¡å‹

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

logging.info("Now FastAPI Platform is valid on ")

class OpenAIChatRequest(BaseModel):
    model: str
    messages: List[dict]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 512
    # ä¸å†è€ƒè™‘æµå¼ï¼Œç›´æ¥ç§»é™¤æˆ–ä¿ç•™éƒ½è¡Œï¼Œè¿™é‡Œä¿ç•™ä½†æ— æ•ˆ
    stream: Optional[bool] = False

def query_lm_studio(model_name, messages, temperature, max_tokens):
    """
    åªåšä¸€æ¬¡æ€§è¿”å›çš„æŸ¥è¯¢ï¼Œä¸å†æ”¯æŒæµå¼ã€‚
    """
    payload = {
        "model": model_name,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": False  # å¼ºåˆ¶å–æ¶ˆæµå¼
    }
    headers = {"Content-Type": "application/json"}

    try:
        response = requests.post(LM_STUDIO_API_URL, json=payload, headers=headers)
        response.raise_for_status()
        json_response = response.json()

        if "choices" not in json_response:
            raise ValueError(f"âŒ API å“åº”é”™è¯¯: {json_response}")

        # åªæ‹¿ä¸€æ¬¡æ€§å®Œæ•´å†…å®¹
        return json_response["choices"][0]["message"]["content"]

    except requests.RequestException as e:
        logging.error(f"âŒ LM Studio æ¨¡å‹ {model_name} å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    except ValueError as ve:
        logging.error(f"âŒ API è¿”å›é”™è¯¯: {ve}")
        raise HTTPException(status_code=500, detail=str(ve))

@app.post("/v1/chat/completions")
async def chat_completions(request: OpenAIChatRequest, background_tasks: BackgroundTasks):
    """OpenAI å…¼å®¹çš„ /v1/chat/completions ç«¯ç‚¹ï¼Œåªèƒ½ä¸€æ¬¡æ€§æ•´å—è¿”å›ã€‚"""

    logging.info("ğŸ”¹ æ”¶åˆ°æ–°è¯·æ±‚ï¼Œå¼€å§‹è®°å¿†å¤„ç†...")
    logging.info(f"ğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    if request.messages:
        logging.info(f"ğŸ“¥ ç”¨æˆ·è¾“å…¥: {request.messages[-1].get('content', '')}")

    # ========= è®°å¿†å¤„ç† ===========
    last_user_msg = None
    # ä»åå¾€å‰æ‰¾ï¼Œæ‰¾åˆ°ç¬¬ä¸€æ¡ role=="user" çš„æ¶ˆæ¯
    for msg in reversed(request.messages):
        if msg["role"] == "user":
            last_user_msg = msg
            break

    if not last_user_msg:
        logging.warning("æ²¡æœ‰å‘ç°ç”¨æˆ·æ¶ˆæ¯ï¼Œè·³è¿‡ MEM å¤„ç†ã€‚")
        mem_reference = ""
    else:
        mem_result = Memory_checking(last_user_msg)
        mem_reference = ""
        mem_distance = ""
        mem_count = 1

        # 2) è¯»å–ç›¸ä¼¼åº¦èŒƒå›´é…ç½®
        MIN_DISTANCE = config.MIN_DISTANCE
        MAX_DISTANCE = config.MAX_DISTANCE

        # 3) å–å‡º IDsï¼Œç”¨äºåç»­å¾ªç¯
        ids_list = mem_result.get("ids", [[]])[0]
        distances_list = mem_result.get("distances", [[]])[0]
        metadatas_list = mem_result.get("metadatas", [[]])[0]

        num_results = len(ids_list)

        # 4) éå†æ‰€æœ‰æ£€ç´¢åˆ°çš„è®°å¿†
        for i in range(num_results):
            # è·å–ç›¸ä¼¼åº¦
            result_distance = distances_list[i] if i < len(distances_list) else float("inf")

            # æ ¹æ®ç›¸ä¼¼åº¦èŒƒå›´è¿‡æ»¤
            if MIN_DISTANCE <= result_distance <= MAX_DISTANCE:
                result_time = ids_list[i]
                # è·å–å…ƒæ•°æ®
                md = metadatas_list[i] if i < len(metadatas_list) else {}
                result_name = md.get("name", "æœªçŸ¥")
                result_source = md.get("source", "æœªçŸ¥")

                # é€šè¿‡ get_memory_with_context() è·å–ä¸Šä¸‹æ–‡
                memory_context = get_memory_with_context(result_time, config.context_size)
                previous_context = memory_context.get("previous", [])
                target_content = memory_context.get("target", {}).get("content", "æœªçŸ¥å†…å®¹")
                next_context = memory_context.get("next", [])

                # å»æ‰æ¯«ç§’
                result_time_to_sec = result_time.split(".")[0]

                # æ‹¼æ¥è¾“å‡º
                mem_reference += f"\nç›¸å…³è®°å¿† {mem_count}:\n"
                mem_count += 1

                # å‰æ–‡
                for prev in previous_context:
                    name_prev = prev.get("name", "æœªçŸ¥")
                    src_prev = prev.get("source", "æœªçŸ¥")
                    cnt_prev = prev.get("content", "")
                    mem_reference += f"{result_time_to_sec}ï¼Œ{name_prev} åœ¨ {src_prev} è¯´ï¼š{cnt_prev}\n"

                # ç›®æ ‡
                mem_reference += f"{result_time_to_sec}ï¼Œ{result_name} åœ¨ {result_source} è¯´ï¼š{target_content}\n"

                # åæ–‡
                for nxt in next_context:
                    name_nxt = nxt.get("name", "æœªçŸ¥")
                    src_nxt = nxt.get("source", "æœªçŸ¥")
                    cnt_nxt = nxt.get("content", "")
                    mem_reference += f"{result_time_to_sec}ï¼Œ{name_nxt} åœ¨ {src_nxt} è¯´ï¼š{cnt_nxt}\n"

                # è®°å½•ç›¸ä¼¼åº¦
                mem_distance += f"(ç›¸ä¼¼åº¦: {result_distance:.3f})\n"

        # å¦‚æœä¸ºç©ºè¯´æ˜æ²¡æœ‰ç¬¦åˆç›¸ä¼¼åº¦çš„è®°å¿†
        if not mem_reference:
            mem_reference = "æ— ç›¸å…³è®°å¿†"

    logging.info(f"MEMè¾“å‡º:\n{mem_reference}")
    logging.info(f"MEMæœ€ç›¸å…³è®°å¿†è·ç¦»:\n{mem_distance}")

    # æŠŠ MEM è¾“å‡ºåˆå¹¶åˆ° system æç¤º
    system_msg = None
    for msg in request.messages:
        if msg["role"] == "system":
            system_msg = msg
            break

    if system_msg is None:
        system_msg = {"role": "system", "content": ""}
        request.messages.insert(0, system_msg)

    system_msg["content"] = config.prompt + f"\nï¼ˆå‚è€ƒè®°å¿†ï¼š{mem_reference}ï¼‰"

    # è°ƒç”¨ LLMï¼Œéæµå¼
    lm_response = query_lm_studio(
        model_name=LLM_Model,
        messages=request.messages,
        temperature=config.temperature,
        max_tokens=config.max_tokens
    )

    logging.info(f"ğŸ“¤ LLMæ¨¡å‹å›å¤: {lm_response}")

    # ä¸€æ¬¡æ€§è¿”å›å®Œæ•´å›ç­”
    response_data = {
        "id": f"chatcmpl-{int(time.time())}",
        "object": "chat.completion",
        "created": int(time.time()),
        "model": LLM_Model,
        "choices": [
            {
                "message": {
                    "role": "assistant",
                    "content": lm_response
                },
                "finish_reason": "stop"
            }
        ]
    }

        # **åœ¨åå°å­˜å‚¨è®°å¿†**
    if last_user_msg:
        background_tasks.add_task(Memory_saving, last_user_msg)
        background_tasks.add_task(Memory_saving, {
            "content": lm_response, "role": "assistant", "name": "éœ“éœ²", "from": ""
        })

    return response_data

